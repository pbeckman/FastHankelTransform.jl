In the following section, we perform a number of numerical experiments to
validate the accuracy and complexity of our method. We close with an application
to computing Fourier transforms of radial kernel functions from spatial
statistics.

\subsection{Comparison to direct evaluation}

We start by empirically verifying the analysis in Theorem~\ref{thm:complexity}.
In order to study the impact of each of the relevant parameters independently,
we take $n$ equispaced points $r_k$ in the interval $[0,\sqrt{10^5}]$ and $m$
equispaced frequencies $\omega_j$ in the interval $[0,p/\sqrt{10^5}]$. First, we
fix $m=10^3$ and $p=10^5$ while increasing $n$. Next, we fix $n=10^3$ and
$p=10^5$, this time increasing $m$. Finally, we fix both $n = m = 10^3$ while
increasing $p$. Figure~\ref{fig:nmp-scaling} shows the CPU time for the NUFHT as
well as for direct summation in each of these scenarios. We observe the linear
or quasilinear scaling expected from Theorem~\ref{thm:complexity} with each of
$n, m,$ and $p$. Note in particular that the NUFHT scales with $p$ while direct
summation does not. Therefore, if a DHT is desired with relatively few points
with a very large space-frequency product, direct summation may give superior
performance, although such circumstances are rare in practice.

\begin{figure}[!t]
  \centering
  \newcommand\twa{0.29cm} \newcommand\tw{0.43cm}
  \begin{subfigure}[b]{0.32\textwidth}
    \begin{tikzpicture}
        \draw (0, 0) node[inner sep=0]
        {\includegraphics[width=\textwidth]{./figures/n_scaling.pdf}}; \draw
        (1.4, 0.1) node {\small $\bO(n)$};
    \end{tikzpicture}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \begin{tikzpicture}
        \draw (0, 0) node[inner sep=0]
        {\includegraphics[width=\textwidth]{./figures/m_scaling.pdf}}; \draw
        (1.4, 0.1) node {\small $\bO(m)$};
    \end{tikzpicture}
  \end{subfigure}
  \hfill 
  \begin{subfigure}[b]{0.32\textwidth}
    \begin{tikzpicture}
        \draw (0, 0) node[inner sep=0]
        {\includegraphics[width=\textwidth]{./figures/p_scaling.pdf}}; \draw
        (1.2, -0.4) node {\small $\bO(p\log p)$};
    \end{tikzpicture}
  \end{subfigure}
  \caption{Scaling with $n$, $m$, and $p$.}
  \label{fig:nmp-scaling}
\end{figure}


Next, we study the more typical scenario where the space-frequency product $p$
grows linearly with $n$, as discussed in Corollary~\ref{cor:complexity}. Here we
study two cases. First, we consider the Fourier-Bessel expansion where $\omega_j
= \mathrm{j}_{\nu, j}$ and $r_k = \mathrm{j}_{\nu, k}/\mathrm{j}_{\nu, n+1}$
with $n = m$. This is the direct analogue of the discrete Fourier transform as
the points and frequencies are the scaled roots of the basis, and the resulting
points and frequencies are quasi-equispaced for small to moderate $\nu$. 

We also consider the case of exponentially distributed points and frequencies
$\omega_j = r_j = 10^{\log_{10}(j) - \log_{10}(n)/2}$ with $n = m$. This is a
somewhat pathological worst case scenario for our algorithm, as the simple
calculation
\begin{equation}
  \sqrt{\frac{\Omega z}{R}} = \argmax_{\frac{z}{R} \leq \omega \leq \Omega} \ (\Omega - \omega)\left(R - \frac{z}{\omega}\right)
\end{equation}
shows that if we subdivide a block with space frequency product $\Omega R$ at a
point $(\omega, r)$ which lies on the curve $\omega r = z$, then the largest
possible space-frequency product $p$ for the resulting lower right asymptotic
block is achieved by taking $\omega$ to be the mid-point of $[z/R, 
\Omega]$ on a log scale. In other words, points and frequencies which are
exponentially distributed result in the highest possible space-frequency product
$p$ for every asymptotic block at every level. From
Theorem~\ref{thm:complexity}, maximizing $p$ drives the cost of the NUFHT. This
distribution of points and frequencies is also challenging because it leads to
equally-sized square blocks at every level, which guarantees that all blocks are
subdivided the maximum number of times before yielding sufficiently small direct
blocks.

Figure~\ref{fig:both-scaling} shows the CPU time needed to evaluate the NUFHT in
the Fourier-Bessel and exponentially-distributed cases with $\nu=0$ and
$\epsilon=10^{-8}$. Both cases eventually demonstrate the expected $\bO(n\log
n)$ scaling. As a result of to the challenges just discussed for the
exponentially-distributed case, its runtime is up to an order of magnitude
slower than the Fourier-Bessel series.

Next, we study the impact of the order $\nu$ and tolerance $\epsilon$ on the
runtime. As $\nu$ increases and as $\epsilon$ decreases, the number of necessary
terms $L$ and $M$ in the local and asymptotic expansions respectively both grow.
From Theorem~\ref{thm:complexity}, we expect the runtime to grow linearly with
$L$ and $M$. Figure~\ref{fig:both-scaling} shows the runtime of our method for
various $\epsilon$ with $\nu=0$ held constant, as well as for multiple $\nu$
with $\epsilon=10^{-8}$ fixed. The scaling of the algorithm is similar in all
cases, while the prefactors vary --- a transform which is accurate to 15 digits
is about an order of magnitude slower than the 4-digit equivalent, and an order
100 transform is almost two orders of magnitude slower the order-0 equivalent.

Finally, we study the relative error in the output $\vct{g}$ as a function of
the desired tolerance $\epsilon$. To do this, we fix $n$ and form a sparse
vector $\vct{f} \in \R^n$ with 1000 nonzero entries whose indices are selected
at random and whose values are independent standard Gaussian. We evaluate the
Fourier-Bessel series with this vector as input using both the NUFHT (giving
$\vct{\tilde{g}}$) and using direct summation (giving $\vct{g}$). We then
compute the the 2-norm relative error $\norm[2]{\vct{g} - \vct{\tilde{g}}} /
\norm[2]{\vct{g}}$. See Figure~\ref{fig:accuracy}. For small transforms with
$n=10^3$, the relative error demonstrates excellent agreement with the tolerance
$\epsilon$ down to $\epsilon = 10^{-13}$ or so. This suggests that the analysis
used in Section~\ref{sec:approx} to determine the necessary number of local and
asymptotic terms is fairly tight. For larger transforms, however, the error
saturates, and regardless of the tolerance $\epsilon$ our method gives at most 9
digits of accuracy for transforms of size $n=10^7$. This is a well-known
limitation of existing NUFFT methods, for which the error generally scales like
$n$ times machine precision \cite[Remark 9]{barnett2019parallel}.

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \begin{tikzpicture}
        \draw (0, 0) node[inner sep=0]
        {\includegraphics[width=\textwidth]{./figures/both_scaling.pdf}}; \draw
        (1.2, 0.2) node {\small $\bO(n\log n)$}; \draw (-0.45, 0.8) node {\small
        $\bO\big(n^2\big)$};
    \end{tikzpicture}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{./figures/tol_scaling.pdf}
  \end{subfigure} 
  \hfill 
  \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{./figures/nu_scaling.pdf}
  \end{subfigure}
  \caption{Scaling with $n$ for $p = \bO(n)$ test cases.}
  \label{fig:both-scaling}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.6\textwidth]{./figures/accuracy.pdf}
  \caption{Relative 2-norm error $\norm[2]{\vct{g} - \vct{\tilde{g}}} /
  \norm[2]{\vct{g}}$ as a function of tolerance $\epsilon$ for a DHT of order
  $0$ for various $n$.}
  \label{fig:accuracy}
\end{figure}


\subsection{Computing Fourier transforms of radial functions}

For radial functions $f(\bm{r}) = f(\norm{\bm{r}})$ in $\R^d$, one can integrate
out the radial variables analytically, reducing the $d$-dimensional Fourier
integral to a single Hankel transform
\begin{align} \label{eq:radial-fourier}
    \hat{f}(\bm{\omega}) 
    = \int_{\R^d} f(\norm{\bm{r}}) e^{i\bm{\omega}^\top \bm{r}} \dif{\bm{r}}
    = \frac{(2\pi)^{\frac{d}{2}}}{\omega^{\frac{d}{2} - 1}} \int_0^\infty f(r) J_{\frac{d}{2} - 1}(\omega r) r^{\frac{d}{2}} \dif{r}.
\end{align}

\subsubsection{Two dimensions}

We compare two methods of computing $\hat{f}$ for the indicator function of the
unit disk $f(r) = \ind{0 \leq r \leq 1}$ to absolute error $\epsilon = 10^{-12}$
at $n$ equispaced points $\omega_j \in [0, \omega_{\text{max}}]$. First, we use
a Gauss-Legendre quadrature rule on $[0,1]$ with nodes $r_k$ and weights $w_k$.
We utilize the NUFHT to compute the resulting sum
\begin{align}
  \hat{f}(\omega) 
  &= 2\pi\int_0^1 f(r) J_0(\omega r) r \dif{r} \\
  &\approx 2\pi \sum_{k=1}^m w_k f(r_k) J_0(\omega r_k) r_k,
\end{align}
doubling the number of nodes $m$ until the error in the computed integral is
less than $\epsilon$. Second, we use a tensor product quadrature rule in polar
coordinates, using the same $m$-point Gauss-Legendre rule in $r$ and a $t$-node
trapezoidal rule in $\theta$. We utilize the NUFFT to compute the resulting
double sum
\begin{align}
  \hat{f}(\omega) 
  &= \int_0^{2\pi} \int_0^1 f(r) e^{i\omega r\cos\theta} r \dif{r} \dif{\theta} \\
  &\approx \frac{2\pi}{t} \sum_{k=1}^{m} \sum_{s=1}^{t} w_k f(r_k) \exp\left\{i\omega r_k\cos\left(\frac{2\pi s}{t}\right)\right\} r_k,
\end{align}
doubling the number of trapezoidal nodes $t$ until the error in the computed
integral is less than $\epsilon$.

If only low frequencies $\omega$ are desired, e.g. $\omega_{\text{max}} = 100$,
the integrands are only mildly oscillatory and few trapezoidal nodes are
required. In combination with the relative ease of amortizing costs in the
NUFFT, the two-dimensional transform is often faster than the NUFHT. However,
for larger $\omega_{\text{max}}$ the integrands become more oscillatory, and
more nodes are needed in each dimension in order to resolve them. In such cases
the fact that $\bO(m^2)$ quadrature nodes are needed in $\R^2$ becomes a
bottleneck, and the necessary NUFFT becomes prohibitively large. 

\red{Quadrature and timing figures}

\subsubsection{Higher dimensions}

Using spherical Bessel function identities \cite[10.47.3,
10.49.2]{olver2010nist}, one can see that for half-integer $\nu$, Hankel's
expansion~\eqref{eq:asymptotic-expansion} is no longer just an asymptotic
expansion, but an exact formula.
% \begin{align} J_{\frac{d}{2}-1}(z) &= \sqrt{\frac{2z}{\pi}}
%   j_{\frac{d-3}{2}}(z) \\
%   &= \sqrt{\frac{2}{\pi z}} \left( \cos\left(\mu\right) \sum_{\ell=0}^{M-1}
%     (-1)^\ell \frac{a_{2\ell}\left(\frac{d}{2}-1\right)}{z^{2\ell}} -
%     \sin\left(\mu\right) \sum_{\ell=0}^{M-1} (-1)^\ell
%     \frac{a_{2\ell+1}\left(\frac{d}{2}-1\right)}{z^{2\ell+1}} \right)
%     \end{align} where $\mu := z - \frac{(2\nu+1)\pi}{4}$. 
For example, the kernel of the integral transform in (\ref{eq:radial-fourier})
for $d = 3$ is
\begin{equation}
  J_{\frac{1}{2}}(z) = \sqrt{\frac{2}{\pi z}} \sin(z),
\end{equation}
and the corresponding NUFHT can be evaluated using a single NUFFT.

The curse of dimensionality demonstrated in the two-dimensional case only
intensifies in higher dimensions, for which the requirement of $\bO(m^d)$
quadrature nodes is infeasible for even small $m$. In contrast, the NUFHT only
ever requires one-dimensional quadrature, and is relatively robust to the
distribution of $\omega_j$ and $r_k$, and to dimension for $d \leq 20$ or so. In
even higher dimensions $d > 20$, the number of terms in the Hankel and Wimp
expansions needed to maintain accurate evaluation increases, and the methods
presented here become inefficient in practice, despite retaining their
quasilinear asymptotic complexity. There exist several alternative asymptotic
expansions in this large $\nu$ regime which can be leveraged for fast evaluation
of $J_\nu(z)$~\cite{heitman2015asymptotics, olver2010nist}.
%\cite[10.19]{heitman2015asymptotics, olver2010nist}
However, the terms in these asymptotics are not sinusoids which can be
efficiently evaluated with the NUFFT, and thus turning them into an
analysis-based fast transform remains an open problem.
