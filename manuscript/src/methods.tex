
\subsection{Subdividing the matrix into blocks by expansion}

Having established error bounds which allow us to automatically select the
number of asymptotic terms $M$, local terms $L$, and crossover point $z$ given a
tolerance $\epsilon$ and order $\nu$, we now discuss how to subdivide the matrix
$\mtx{A}$ into three sets of blocks, each of which can be efficiently applied to
a vector as described above:
\begin{list}{$\circ$}{}
    \item Local blocks $\mathscr{L} = \big\{ (j_0:j_1, k_0:k_1) \ | \ \omega_j
    r_k \leq z \ \forall \ j_0 \leq j \leq j_1, \ k_0 \leq k \leq k_1 \big\}$
    \item Asymptotic blocks $\mathscr{A} = \big\{ (j_0:j_1, k_0:k_1) \ | \
    \omega_j r_k > z \ \forall \ j_0 \leq j \leq j_1, \ k_0 \leq k \leq k_1
    \big\}$
    \item Direct blocks $\mathscr{D}$ which are small enough that no fast
    expansion is needed
\end{list}

In order to determine a subdivision of $\mtx{A}$ into blocks of these three
types, we initialize a set of \textit{mixed} blocks $\mathscr{M} = \{(1:m,
1:n)\}$, which contains a mix of local and asymptotic entries. We then chose an
index pair $(j,k)$ such that $\omega_j r_k \approx z$. This index subdivides the
block into four new sub-blocks with $(j,k)$ at the center, so that the upper
left block can be applied using the local expansion and is appended to
$\mathscr{L}$, the lower right block using the asymptotic expansion and is
appended to $\mathscr{A}$. 

The remaining lower left and upper right blocks each still contain a mix of
local and asymptotic entries. If they are of sufficiently small size $m_b \times
n_b$ with $m_b n_b < \texttt{min\_size} := 1024$, they can be evaluated directly
and are appended to $\mathscr{D}$. Otherwise they are appended back to
$\mathscr{M}$, and we continue the subdivision process recursively. 

This method yields a valid partition for any choice of $(j,k)$, but for
efficiency these indices are chosen to maximize the number of matrix entries
which can be applied using a fast expansion, i.e. the sizes of the upper left
and lower right blocks. This is done by solving the following constrained
optimization problem
\begin{align}
    (j,k) 
    &\ = \textsc{SplitIndices}(r_1,\dots,r_n, \omega_1,\dots,\omega_m, z) \\
    &:= \left\{
        \begin{array}{r@{\quad } l}
        \text{argmax} & (j-j_0)(k_1-k) + (j_1-j)(k-k_0)   \\
        \text{subject to} & j_0 \leq j \leq j_1 \\ 
        & k_0 \leq k \leq k_1 \\ 
        & \omega_j r_k \leq z
        \end{array}
    \right. \label{eq:subdiv-optim}
\end{align}
This problem can be solved exactly in \red{$\bO(j_1 - j_0 + k_1 - k_0)$?} time
using \red{bisection?}. However, computing the exact optimal splitting indices
for every box gives a negligible speedup overall compared to a simpler,
quasi-optimal scheme. In practice it is sufficient to choose a small number of
equispaced indices $j \in j_0:j_1$, compute the corresponding $k = \argmax \{k \
| \ r_k \leq \frac{z}{\omega_j}\}$ for each $j$, and choose $(j,k)$ as the pair
which minimizes the objective function of (\ref{eq:subdiv-optim}) among this
small collection.

\begin{algorithm2e}[t]
    \caption{Block subdivision of Hankel transform
    matrix}\label{alg:subdivision}
    \include{src/subdiv-algo.tex}
\end{algorithm2e}

\begin{algorithm2e}[t]
    \caption{Nonuniform fast Hankel transform}\label{alg:nufht}
    \include{src/nufht-algo.tex}
\end{algorithm2e}

\subsection{Complexity analysis}

We now analyze the computational complexity of the proposed approach. In order
to do so, we must first comment on the complexity of the NUFFT, which is an
important subroutine in our method. Most analysis-based NUFFT codes ---
including the FINUFFT library \cite{barnett2019parallel} which we use in our
NUFHT implementation --- consist of three steps. First, delta masses centered at
each non-uniform point are convolved with a \textit{spreading function} which
smears them onto a fine $N$-point uniform grid. Then, a standard equispaced FFT
is computed on the fine grid. Finally, a diagonal de-convolution with the
Fourier transform of the spreading function is applied to reverse the effect of
the original smearing. For a more complete description of this NUFFT method, see
\cite{dutt1993fast,greengard2004accelerating,barnett2019parallel}. For $n$
points $r_k$ and $m$ frequencies $\omega_j$, spreading of each input point is
$\bO(n)$, the FFT on the fine grid is $\bO(N\log N)$, and the deconvolution of
each output frequency is $\bO(m)$. 

\begin{theorem} \label{thm:complexity} Take $\omega_1 < \dots < \omega_m \in
    [0,\infty)$ and $r_1 < \dots < r_n \in [0,\infty)$ and define the
    space-frequency product $p := (\omega_m - \omega_1)(r_n - r_1)$. Then the
    complexity of computing the NUFHT of order $\nu$ to tolerance $\epsilon$
    using Algorithm \ref{alg:nufht} is 
    $$\bO\Big((L + M)(m + n) \log \min(n,m) + Mp\log p\Big)$$ where $L$ and $M$
    are the number of local and asymptotic terms respectively chosen according
    to $\nu$ and $\epsilon$.
\end{theorem}

\begin{proof}
    Take $z = z_{\nu, \epsilon}^M$. If $\omega_j r_k \leq z$ for all
    $j=1,\dots,n$ and $k=1,\dots,m$ then only the $L$-term low-rank local
    expansion is used, which can be applied in $\bO(L(m + n))$ time. If instead
    $\omega_j r_k > z$ everywhere, then only the $M$-term asymptotic expansion
    is used, which can be applied using the Type-III NUFFT in $\bO(M(m + n +
    p\log p))$ complexity.

    Otherwise consider the case where $\mtx{A}$ contains both local and
    asymptotic entries. Without loss of generality assume $\omega_1 \leq
    \frac{z}{r_n} < \omega_2$ and $r_1 \leq \frac{z}{\omega_m} < r_2$, because
    otherwise we have blocks which can be evaluated using a single expansion as
    just described without affecting the complexity.

    After step $\ell$ of subdividing every mixed block, we obtain $2^{\ell}$ new
    mixed blocks, $2^{\ell-1}$ new local blocks, and $2^{\ell-1}$ new asymptotic
    blocks. Let the local blocks be of size $m_{\ell,b}^{(\text{loc})} \times
    n_{\ell,b}^{(\text{loc})}$ for $b = 1,\dots,2^{\ell-1}$. Then
    $\sum_{b=1}^{2^{\ell-1}} m_{\ell,b}^{(\text{loc})} \leq m$ and
    $\sum_{b=1}^{2^{\ell-1}} n_{\ell,b}^{(\text{loc})} \leq n$. An analogous
    fact holds for the asymptotic blocks. The number of levels
    $N_{\text{level}}$ scales like $\bO(\log\min(n,m))$. 
    
    Therefore, the total cost of local evaluation is 
    \begin{align}
        \sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} \bO\left(L\left(m_{\ell,b}^{(\text{loc})} + n_{\ell,b}^{(\text{loc})}\right)\right)
        &= \sum_{\ell=1}^{N_{\text{level}}} \bO(L(m + n)) \\
        &= \bO\big(L (m + n) \log \min(n,m)\big).
    \end{align}
    Let $p_{\ell,b}$ be the space-frequency product of box $b$ at level $\ell$.
    The total space frequency product $p$ is the area of the rectangle $R :=
    [\omega_1, \omega_m] \times [r_1, r_n]$, and all asymptotic boxes occupy
    disjoint sub-rectangles of $R$. Therefore, by a simple geometric argument
    the sum of their areas is bounded by the area of $R$, so that
    $\sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} p_{\ell,b} \leq
    p$. Then by H\"older's inequality we obtain 
    \begin{align} 
        \sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} p_{\ell,b} \log p_{\ell,b}
        \leq \left( \sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} p_{\ell,b} \right) \left(\max_{\ell,b} \log p_{\ell,b} \right) 
        \leq p \log p.
    \end{align}
    The total cost of asymptotic evaluation via the Type-III NUFFT is therefore
    \begin{align}
        & \sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} \bO\left(M\left(m_{\ell,b}^{(\text{asy})} + n_{\ell,b}^{(\text{asy})} + p_{\ell,b}^{(\text{asy})} \log p_{\ell,b}^{(\text{asy})} \right)\right) \\
        &\hspace*{0.2\textwidth} = \sum_{\ell=1}^{N_{\text{level}}} \bO(M(m + n)) + \sum_{\ell=1}^{N_{\text{level}}} \sum_{b=1}^{2^{\ell-1}} \bO\Big(M\Big(p_{\ell,b}^{(\text{asy})} \log p_{\ell,b}^{(\text{asy})}\Big)\Big) \\
        &\hspace*{0.2\textwidth} = \bO\big(M (m + n) \log \min(n,m) + Mp\log p\big).
    \end{align}
    We subdivide until all direct blocks are all of size $m_b \times n_b$ with
    $m_bn_b = \bO(1)$. Thus the cost of computing the dense matvec with each
    direct block is $\bO(1)$, and the number of direct blocks is $\bO(m + n)$.
    Therefore the total direct evaluation cost is $\bO(m + n)$. Summing the cost
    of local, asymptotic, and direct evaluation gives the result.
\end{proof}

In typical applications the domain of integration $[0,R]$ is fixed by e.g. the
support of the function $f$ whose Fourier transform is desired, and the maximum
frequency at which the transform is computed grows with $n$. The following
corollary studies this common scenario, which includes Schl\"omilch and
Fourier-Bessel expansions. For notational conciseness, we consider the number of
terms $L$ and $M$ in each expansion as constants here.
\begin{corollary} \label{cor:complexity} Take $\omega_1 < \dots < \omega_n \in
    [0,\infty)$ and $r_1 < \dots < r_n \in [0,\infty)$ such that the
    space-frequency product $p = \bO(n)$. Then the complexity of computing the
    NUFHT using Algorithm \ref{alg:nufht} is $\bO(n\log n)$.
\end{corollary}

\begin{remark}
    For local blocks away from the origin, one can prove that the
    $\epsilon$-rank of blocks of $\mtx{A}$ is actually significantly lower than
    the $L$ used in the Wimp expansion due to the complementary low-rank or
    butterfly property of the Hankel transform. However, the evaluation of local
    blocks is neither the bottleneck asymptotically nor in practice, so we do
    not pursue an optimal-rank expansion in this regime.
\end{remark}